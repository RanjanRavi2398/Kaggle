{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport random\n\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom matplotlib import pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\nfrom tqdm import tqdm\nfrom skimage.exposure import rescale_intensity\nfrom skimage.io import imread, imsave\nfrom skimage.transform import resize, rescale, rotate\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose\nimport glob\nimport random\nimport time\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid import ImageGrid\nimport cv2\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Global Variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path to all data\nDATA_PATH = \"/kaggle/input/lgg-mri-segmentation/kaggle_3m/\"\n\n# File path line length images for later sorting\nBASE_LEN = 89 # len(/kaggle/input/lgg-mri-segmentation/kaggle_3m/TCGA_DU_6404_19850629/TCGA_DU_6404_19850629_ <-!!!43.tif)\nEND_IMG_LEN = 4 # len(/kaggle/input/lgg-mri-segmentation/kaggle_3m/TCGA_DU_6404_19850629/TCGA_DU_6404_19850629_43 !!!->.tif)\nEND_MASK_LEN = 9 # (/kaggle/input/lgg-mri-segmentation/kaggle_3m/TCGA_DU_6404_19850629/TCGA_DU_6404_19850629_43 !!!->_mask.tif)\n\n# img size\nIMG_SIZE = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Raw data\ndata_map = []\nfor sub_dir_path in glob.glob(DATA_PATH+\"*\"):\n    if os.path.isdir(sub_dir_path):\n        dirname = sub_dir_path.split(\"/\")[-1]\n        for filename in os.listdir(sub_dir_path):\n            image_path = sub_dir_path + \"/\" + filename\n            data_map.extend([dirname, image_path])\n    else:\n        print(\"This is not a dir:\", sub_dir_path)\n        \n        \ndf = pd.DataFrame({\"dirname\" : data_map[::2],\n                  \"path\" : data_map[1::2]})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Masks/Not masks\ndf_imgs = df[~df['path'].str.contains(\"mask\")]\ndf_masks = df[df['path'].str.contains(\"mask\")]\n\n# Data sorting\nimgs = sorted(df_imgs[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_IMG_LEN]))\nmasks = sorted(df_masks[\"path\"].values, key=lambda x : int(x[BASE_LEN:-END_MASK_LEN]))\n\n# Sorting check\nidx = random.randint(0, len(imgs)-1)\nprint(\"Path to the Image:\", imgs[idx], \"\\nPath to the Mask:\", masks[idx])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final dataframe\ndf = pd.DataFrame({\"patient\": df_imgs.dirname.values,\n                       \"image_path\": imgs,\n                   \"mask_path\": masks})\n\n\n# Adding A/B column for diagnosis\ndef positiv_negativ_diagnosis(mask_path):\n    value = np.max(cv2.imread(mask_path))\n    if value > 0 : return 1\n    else: return 0\n\ndf[\"diagnosis\"] = df[\"mask_path\"].apply(lambda m: positiv_negativ_diagnosis(m))\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot\nax = df.diagnosis.value_counts().plot(kind='bar',\n                                      stacked=True,\n                                      figsize=(10, 6),\n                                     color=[\"violet\", \"lightseagreen\"])\n\n\nax.set_xticklabels([\"Positive\", \"Negative\"], rotation=45, fontsize=12);\nax.set_ylabel('Total Images', fontsize = 12)\nax.set_title(\"Distribution of data grouped by diagnosis\",fontsize = 18, y=1.05)\n\n# Annotate\nfor i, rows in enumerate(df.diagnosis.value_counts().values):\n    ax.annotate(int(rows), xy=(i, rows-12), \n                rotation=0, color=\"white\", \n                ha=\"center\", verticalalignment='bottom', \n                fontsize=15, fontweight=\"bold\")\n    \nax.text(1.2, 2550, f\"Total {len(df)} images\", size=15,\n        color=\"black\",\n         ha=\"center\", va=\"center\",\n         bbox=dict(boxstyle=\"round\",\n                   fc=(\"lightblue\"),\n                   ec=(\"black\"),\n                   )\n         );\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data\npatients_by_diagnosis = df.groupby(['patient', 'diagnosis'])['diagnosis'].size().unstack().fillna(0)\npatients_by_diagnosis.columns = [\"Positive\", \"Negative\"]\n\n# Plot\nax = patients_by_diagnosis.plot(kind='bar',stacked=True,\n                                figsize=(18, 10),\n                                color=[\"mediumvioletred\", \"springgreen\"], \n                                alpha=0.9)\nax.legend(fontsize=20);\nax.set_xlabel('Patients',fontsize = 20)\nax.set_ylabel('Total Images', fontsize = 20)\nax.set_title(\"Distribution of data grouped by patient and diagnosis\",fontsize = 25, y=1.005)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data\nsample_yes_df = df[df[\"diagnosis\"] == 1].sample(5).image_path.values\nsample_no_df = df[df[\"diagnosis\"] == 0].sample(5).image_path.values\n\nsample_imgs = []\nfor i, (yes, no) in enumerate(zip(sample_yes_df, sample_no_df)):\n    yes = cv2.resize(cv2.imread(yes), (IMG_SIZE, IMG_SIZE))\n    no = cv2.resize(cv2.imread(no), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([yes, no])\n\n\nsample_yes_arr = np.vstack(np.array(sample_imgs[::2]))\nsample_no_arr = np.vstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(1, 4),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_yes_arr)\ngrid[0].set_title(\"Positive\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_no_arr)\ngrid[1].set_title(\"Negative\", fontsize=15)\ngrid[1].axis(\"off\")\n\ngrid[2].imshow(sample_yes_arr[:,:,0], cmap=\"hot\")\ngrid[2].set_title(\"Positive\", fontsize=15)\ngrid[2].axis(\"off\")\ngrid[3].imshow(sample_no_arr[:,:,0], cmap=\"hot\")\ngrid[3].set_title(\"Negative\", fontsize=15)\ngrid[3].axis(\"off\")#set_title(\"No\", fontsize=15)\n\n# annotations\nplt.figtext(0.36,0.90,\"Original\", va=\"center\", ha=\"center\", size=20)\nplt.figtext(0.66,0.90,\"With hot colormap\", va=\"center\", ha=\"center\", size=20)\nplt.suptitle(\"Brain MRI Images for Brain Tumor Detection\\nLGG Segmentation Dataset\", y=.95, fontsize=30, weight=\"bold\")\n\n# save and show\nplt.savefig(\"dataset.png\", bbox_inches='tight', pad_inches=0.2, transparent=True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Samples of Images and masks with positive diagonsis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data\nsample_df = df[df[\"diagnosis\"] == 1].sample(5).values\nsample_imgs = []\nfor i, data in enumerate(sample_df):\n    #print(data)\n    img = cv2.resize(cv2.imread(data[1]), (IMG_SIZE, IMG_SIZE))\n    mask = cv2.resize(cv2.imread(data[2]), (IMG_SIZE, IMG_SIZE))\n    sample_imgs.extend([img, mask])\n\n\nsample_imgs_arr = np.hstack(np.array(sample_imgs[::2]))\nsample_masks_arr = np.hstack(np.array(sample_imgs[1::2]))\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 1),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(sample_imgs_arr)\ngrid[0].set_title(\"Images\", fontsize=15)\ngrid[0].axis(\"off\")\ngrid[1].imshow(sample_masks_arr)\ngrid[1].set_title(\"Masks\", fontsize=15, y=0.9)\ngrid[1].axis(\"off\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DataGenerator and Data Augmentation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor, ToTensorV2\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BrainMriDataset(Dataset):\n    def __init__(self, df, transforms):\n        \n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.df.iloc[idx, 1])\n        mask = cv2.imread(self.df.iloc[idx, 2], 0)\n\n        augmented = self.transforms(image=image, \n                                    mask=mask)\n \n        image = augmented['image']\n        mask = augmented['mask']   \n        \n        return image, mask\n    \n        # unnormilize mask\n        #mask = torch.clamp(mask.float(), min=0, max=1)\n        #mask = torch.ceil(mask)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform**"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATCH_SIZE = 128#256\n\nstrong_transforms = A.Compose([\n    A.RandomResizedCrop(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n    \n    # Pixels\n    A.RandomBrightnessContrast(p=0.5),\n    A.RandomGamma(p=0.25),\n    A.IAAEmboss(p=0.25),\n    A.Blur(p=0.01, blur_limit = 3),\n    \n    # Affine\n    A.OneOf([\n        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(p=0.5),\n        A.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n    ], p=0.8),\n    \n    \n    A.Normalize(p=1.0),\n    #https://albumentations.readthedocs.io/en/latest/api/pytorch.html?highlight=ToTensor#albumentations.pytorch.transforms.ToTensor\n    ToTensor(),\n])\n\n\ntransforms = A.Compose([\n    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.Transpose(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n\n    \n    \n    A.Normalize(p=1.0),\n    ToTensor(),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split data on train val test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split df into train_df and val_df\ntrain_df, val_df = train_test_split(df, stratify=df.diagnosis, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\n# Split train_df into train_df and test_df\ntrain_df, test_df = train_test_split(train_df, stratify=train_df.diagnosis, test_size=0.15)\ntrain_df = train_df.reset_index(drop=True)\n\n#train_df = train_df[:1000]\nprint(f\"Train: {train_df.shape} \\nVal: {val_df.shape} \\nTest: {test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\ntrain_dataset = BrainMriDataset(df=train_df, transforms=transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n# val\nval_dataset = BrainMriDataset(df=val_df, transforms=transforms)\nval_dataloader = DataLoader(val_dataset, batch_size=26, num_workers=4, shuffle=True)\n\n#test\ntest_dataset = BrainMriDataset(df=test_df, transforms=transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=26, num_workers=4, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Augmentation Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_aug(inputs, nrows=5, ncols=5, image=True):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0., hspace=0.)\n    i_ = 0\n    \n    if len(inputs) > 25:\n        inputs = inputs[:25]\n        \n    for idx in range(len(inputs)):\n    \n        # normalization\n        if image is True:           \n            img = inputs[idx].numpy().transpose(1,2,0)\n            mean = [0.485, 0.456, 0.406]\n            std = [0.229, 0.224, 0.225] \n            img = (img*std+mean).astype(np.float32)\n        else:\n            img = inputs[idx].numpy().astype(np.float32)\n            img = img[0,:,:]\n        \n        #plot\n        #print(img.max(), len(np.unique(img)))\n        plt.subplot(nrows, ncols, i_+1)\n        plt.imshow(img); \n        plt.axis('off')\n \n        i_ += 1\n        \n    return plt.show()\n\n    \nimages, masks = next(iter(train_dataloader))\nprint(images.shape, masks.shape)\n\nshow_aug(images)\nshow_aug(masks, image=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Selection**"},{"metadata":{},"cell_type":"markdown","source":"**UNet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n        nn.ReLU(inplace=True))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n                \n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n\n        self.maxpool = nn.MaxPool2d(2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n        \n        self.conv_up3 = double_conv(256 + 512, 256)\n        self.conv_up2 = double_conv(128 + 256, 128)\n        self.conv_up1 = double_conv(128 + 64, 64)\n        \n        self.last_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n        \n        \n    def forward(self, x):\n        # Batch - 1d tensor.  N_channels - 1d tensor, IMG_SIZE - 2d tensor.\n        # Example: x.shape >>> (10, 3, 256, 256).\n        \n        conv1 = self.conv_down1(x)  # <- BATCH, 3, IMG_SIZE  -> BATCH, 64, IMG_SIZE..\n        x = self.maxpool(conv1)     # <- BATCH, 64, IMG_SIZE -> BATCH, 64, IMG_SIZE 2x down.\n        conv2 = self.conv_down2(x)  # <- BATCH, 64, IMG_SIZE -> BATCH,128, IMG_SIZE.\n        x = self.maxpool(conv2)     # <- BATCH, 128, IMG_SIZE -> BATCH, 128, IMG_SIZE 2x down.\n        conv3 = self.conv_down3(x)  # <- BATCH, 128, IMG_SIZE -> BATCH, 256, IMG_SIZE.\n        x = self.maxpool(conv3)     # <- BATCH, 256, IMG_SIZE -> BATCH, 256, IMG_SIZE 2x down.\n        x = self.conv_down4(x)      # <- BATCH, 256, IMG_SIZE -> BATCH, 512, IMG_SIZE.\n        x = self.upsample(x)        # <- BATCH, 512, IMG_SIZE -> BATCH, 512, IMG_SIZE 2x up.\n        \n        #(Below the same)                                 N this       ==        N this.  Because the first N is upsampled.\n        x = torch.cat([x, conv3], dim=1) # <- BATCH, 512, IMG_SIZE & BATCH, 256, IMG_SIZE--> BATCH, 768, IMG_SIZE.\n        \n        x = self.conv_up3(x) #  <- BATCH, 768, IMG_SIZE --> BATCH, 256, IMG_SIZE. \n        x = self.upsample(x)  #  <- BATCH, 256, IMG_SIZE -> BATCH,  256, IMG_SIZE 2x up.   \n        x = torch.cat([x, conv2], dim=1) # <- BATCH, 256,IMG_SIZE & BATCH, 128, IMG_SIZE --> BATCH, 384, IMG_SIZE.  \n\n        x = self.conv_up2(x) # <- BATCH, 384, IMG_SIZE --> BATCH, 128 IMG_SIZE. \n        x = self.upsample(x)   # <- BATCH, 128, IMG_SIZE --> BATCH, 128, IMG_SIZE 2x up.     \n        x = torch.cat([x, conv1], dim=1) # <- BATCH, 128, IMG_SIZE & BATCH, 64, IMG_SIZE --> BATCH, 192, IMG_SIZE.  \n        \n        x = self.conv_up1(x) # <- BATCH, 128, IMG_SIZE --> BATCH, 64, IMG_SIZE.\n        \n        out = self.last_conv(x) # <- BATCH, 64, IMG_SIZE --> BATCH, n_classes, IMG_SIZE.\n        out = torch.sigmoid(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unet = UNet(n_classes=1).to(device)\noutput = unet(torch.randn(1,3,256,256).to(device))\nprint(\"\",output.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvReluUpsample(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.make_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        \n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = self.make_upsample(x)\n        return x\n\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [ConvReluUpsample(in_channels, out_channels, upsample=bool(n_upsamples))]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(ConvReluUpsample(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FPN(nn.Module):\n\n    def __init__(self, n_classes=1, \n                 pyramid_channels=256, \n                 segmentation_channels=256):\n        super().__init__()\n         \n        # Bottom-up layers\n        self.conv_down1 = double_conv(3, 64)\n        self.conv_down2 = double_conv(64, 128)\n        self.conv_down3 = double_conv(128, 256)\n        self.conv_down4 = double_conv(256, 512)        \n        self.conv_down5 = double_conv(512, 1024)   \n        self.maxpool = nn.MaxPool2d(2)\n        \n        # Top layer\n        self.toplayer = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n        \n        # Segmentation block layers\n        self.seg_blocks = nn.ModuleList([\n            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n            for n_upsamples in [0, 1, 2, 3]\n        ])\n        \n        # Last layer\n        self.last_conv = nn.Conv2d(256, n_classes, kernel_size=1, stride=1, padding=0)\n        \n    def upsample_add(self, x, y):\n        _,_,H,W = y.size()\n        upsample = nn.Upsample(size=(H,W), mode='bilinear', align_corners=True) \n        \n        return upsample(x) + y\n    \n    def upsample(self, x, h, w):\n        sample = nn.Upsample(size=(h, w), mode='bilinear', align_corners=True)\n        return sample(x)\n        \n    def forward(self, x):\n        \n        # Bottom-up\n        c1 = self.maxpool(self.conv_down1(x))\n        c2 = self.maxpool(self.conv_down2(c1))\n        c3 = self.maxpool(self.conv_down3(c2))\n        c4 = self.maxpool(self.conv_down4(c3))\n        c5 = self.maxpool(self.conv_down5(c4)) \n        \n        # Top-down\n        p5 = self.toplayer(c5) \n        p4 = self.upsample_add(p5, self.latlayer1(c4)) \n        p3 = self.upsample_add(p4, self.latlayer2(c3))\n        p2 = self.upsample_add(p3, self.latlayer3(c2)) \n        \n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        \n        # Segmentation\n        _, _, h, w = p2.size()\n        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p2, p3, p4, p5])]\n        \n        out = self.upsample(self.last_conv(sum(feature_pyramid)), 4 * h, 4 * w)\n        \n        out = torch.sigmoid(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpn = FPN().to(device)\noutput = fpn(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.models import resnext50_32x4d\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, padding):\n        super().__init__()\n\n        self.convrelu = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.convrelu(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = ConvRelu(in_channels, in_channels // 4, 1, 0)\n        \n        self.deconv = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=4,\n                                          stride=2, padding=1, output_padding=0)\n        \n        self.conv2 = ConvRelu(in_channels // 4, out_channels, 1, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.deconv(x)\n        x = self.conv2(x)\n\n        return x\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNeXtUNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.base_model = resnext50_32x4d(pretrained=True)\n        self.base_layers = list(self.base_model.children())\n        filters = [4*64, 4*128, 4*256, 4*512]\n        \n        # Down\n        self.encoder0 = nn.Sequential(*self.base_layers[:3])\n        self.encoder1 = nn.Sequential(*self.base_layers[4])\n        self.encoder2 = nn.Sequential(*self.base_layers[5])\n        self.encoder3 = nn.Sequential(*self.base_layers[6])\n        self.encoder4 = nn.Sequential(*self.base_layers[7])\n\n        # Up\n        self.decoder4 = DecoderBlock(filters[3], filters[2])\n        self.decoder3 = DecoderBlock(filters[2], filters[1])\n        self.decoder2 = DecoderBlock(filters[1], filters[0])\n        self.decoder1 = DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.last_conv0 = ConvRelu(256, 128, 3, 1)\n        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)\n                       \n        \n    def forward(self, x):\n        # Down\n        x = self.encoder0(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Up + sc\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n        #print(d1.shape)\n\n        # final classifier\n        out = self.last_conv0(d1)\n        out = self.last_conv1(out)\n        out = torch.sigmoid(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rx50 = ResNeXtUNet(n_classes=1).to(device)\noutput = rx50(torch.randn(1,3,256,256).to(device))\nprint(output.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n\n    return intersection / union\n\n# Metric check\ndice_coef_metric(np.array([0., 0.9]), \n                 np.array([0., 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef_loss(inputs, target):\n    smooth = 1.0\n    intersection = 2.0 * ((target * inputs).sum()) + smooth\n    union = target.sum() + inputs.sum() + smooth\n\n    return 1 - (intersection / union)\n\n\ndef bce_dice_loss(inputs, target):\n    dicescore = dice_coef_loss(inputs, target)\n    bcescore = nn.BCELoss()\n    bceloss = bcescore(inputs, target)\n\n    return bceloss + dicescore\n\n# loss check\nbce_dice_loss(torch.tensor([0.7, 1., 1.]), \n              torch.tensor([1.,1.,1.]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model_name, model, train_loader, val_loader, train_loss, optimizer, lr_scheduler, num_epochs):  \n    \n    print(model_name)\n    loss_history = []\n    train_history = []\n    val_history = []\n\n    for epoch in range(num_epochs):\n        model.train() # Enter train mode\n        \n        losses = []\n        train_iou = []\n                \n        if lr_scheduler:\n            \n            warmup_factor = 1.0 / 100\n            warmup_iters = min(100, len(train_loader) - 1)\n            lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n        \n        \n        for i_step, (data, target) in enumerate(train_loader):\n            data = data.to(device)\n            target = target.to(device)\n                      \n            outputs = model(data)\n            \n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n            \n            train_dice = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            \n            loss = train_loss(outputs, target)\n            \n            losses.append(loss.item())\n            train_iou.append(train_dice)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            if lr_scheduler:\n                lr_scheduler.step()\n \n        #torch.save(model.state_dict(), f'{model_name}_{str(epoch)}_epoch.pt')\n        val_mean_iou = compute_iou(model, val_loader)\n        \n        loss_history.append(np.array(losses).mean())\n        train_history.append(np.array(train_iou).mean())\n        val_history.append(val_mean_iou)\n        \n        print(\"Epoch [%d]\" % (epoch))\n        print(\"Mean loss on train:\", np.array(losses).mean(), \n              \"\\nMean DICE on train:\", np.array(train_iou).mean(), \n              \"\\nMean DICE on validation:\", val_mean_iou)\n        \n    return loss_history, train_history, val_history\n\n\ndef compute_iou(model, loader, threshold=0.3):\n    \"\"\"\n    Computes accuracy on the dataset wrapped in a loader\n    \n    Returns: accuracy as a float value between 0 and 1\n    \"\"\"\n    #model.eval()\n    valloss = 0\n    \n    with torch.no_grad():\n\n        for i_step, (data, target) in enumerate(loader):\n            \n            data = data.to(device)\n            target = target.to(device)\n            #prediction = model(x_gpu)\n            \n            outputs = model(data)\n           # print(\"val_output:\", outputs.shape)\n\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            picloss = dice_coef_metric(out_cut, target.data.cpu().numpy())\n            valloss += picloss\n\n        #print(\"Threshold:  \" + str(threshold) + \"  Validation DICE score:\", valloss / i_step)\n\n    return valloss / i_step\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimizers\nunet_optimizer = torch.optim.Adamax(unet.parameters(), lr=1e-3)\nfpn_optimizer = torch.optim.Adamax(fpn.parameters(), lr=1e-3)\nrx50_optimizer = torch.optim.Adam(rx50.parameters(), lr=5e-4)\n\n# lr_scheduler\ndef warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n    def f(x):\n        if x >= warmup_iters:\n            return 1\n        alpha = float(x) / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_ep = 10                                                                                                  \n# Train UNet\n#unet_lh, unet_th, unet_vh = train_model(\"Vanila_UNet\", unet, train_dataloader, val_dataloader, bce_dice_loss, unet_optimizer, False, 20) \n\n# Train FPN\n#fpn_lh, fpn_th, fpn_vh = train_model(\"FPN\", fpn, train_dataloader, val_dataloader, bce_dice_loss, fpn_optimizer, False, 20)#\n\n# Train ResNeXt50\nrx50_lh, rx50_th, rx50_vh = train_model(\"ResNeXt50\", rx50, train_dataloader, val_dataloader, bce_dice_loss, rx50_optimizer, False, num_ep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model_history(model_name,\n                        train_history, val_history, \n                        num_epochs):\n    \n    x = np.arange(num_epochs)\n\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n\n    plt.title(f\"{model_name}\", fontsize=15)\n    plt.legend(fontsize=12)\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"DICE\", fontsize=15)\n\n    fn = str(int(time.time())) + \".png\"\n    plt.show()\n    #plt.savefig(fn, bbox_inches='tight', pad_inches=0.2)\n    #plt.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_model_history(\"Vanilla UNet\", unet_th, unet_vh, 20)\n#plot_model_history(\"FPN\", fpn_th, fpn_vh, 20)\nplot_model_history(\"UNet with ResNeXt50 backbone\", rx50_th, rx50_vh, num_ep)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_iou = compute_iou(unet, test_dataloader)\n#print(f\"\"\"Vanilla UNet\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\n#test_iou = compute_iou(fpn, test_dataloader)\n#print(f\"\"\"FPN\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\n\ntest_iou = compute_iou(rx50, test_dataloader)\nprint(f\"\"\"ResNext50\\nMean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")\nResNext50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Test Sample**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# image\ntest_sample = test_df[test_df[\"diagnosis\"] == 1].sample(1).values[0]\nimage = cv2.resize(cv2.imread(test_sample[1]), (128, 128))\n\n#mask\nmask = cv2.resize(cv2.imread(test_sample[2]), (128, 128))\n\n# pred\npred = torch.tensor(image.astype(np.float32) / 255.).unsqueeze(0).permute(0,3,1,2)\npred = rx50(pred.to(device))\npred = pred.detach().cpu().numpy()[0,0,:,:]\n\n# pred with tshd\npred_t = np.copy(pred)\npred_t[np.nonzero(pred_t < 0.3)] = 0.0\npred_t[np.nonzero(pred_t >= 0.3)] = 255.#1.0\npred_t = pred_t.astype(\"uint8\")\n\n# plot\nfig, ax = plt.subplots(nrows=2,  ncols=2, figsize=(10, 10))\n\nax[0, 0].imshow(image)\nax[0, 0].set_title(\"image\")\nax[0, 1].imshow(mask)\nax[0, 1].set_title(\"mask\")\nax[1, 0].imshow(pred)\nax[1, 0].set_title(\"prediction\")\nax[1, 1].imshow(pred_t)\nax[1, 1].set_title(\"prediction with threshold\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_samples = test_df[test_df[\"diagnosis\"] == 1].sample(105).values\n\n\ndef batch_preds_overlap(model, samples):\n    \"\"\"\n    Computes prediction on the dataset\n    \n    Returns: list with images overlapping with predictions\n    \n    \"\"\"\n    prediction_overlap = []\n    #model.eval():\n    for test_sample in samples:\n\n         # sample\n        image = cv2.resize(cv2.imread(test_sample[1]),(128, 128))\n        image =  image / 255.\n\n        # gt\n        ground_truth = cv2.resize(cv2.imread(test_sample[2], 0), (128, 128)).astype(\"uint8\")\n\n        # pred\n        prediction = torch.tensor(image).unsqueeze(0).permute(0,3,1,2)\n        prediction = model(prediction.to(device).float())\n        prediction = prediction.detach().cpu().numpy()[0,0,:,:]\n\n        prediction[np.nonzero(prediction < 0.3)] = 0.0\n        prediction[np.nonzero(prediction >= 0.3)] = 255.#1.0\n        prediction = prediction.astype(\"uint8\")\n\n        # overlap \n        original_img = cv2.resize(cv2.imread(test_sample[1]),(128, 128))\n\n        _, thresh_gt = cv2.threshold(ground_truth, 127, 255, 0)\n        _, thresh_p = cv2.threshold(prediction, 127, 255, 0)\n        contours_gt, _ = cv2.findContours(thresh_gt, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        contours_p, _ = cv2.findContours(thresh_p, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        overlap_img = cv2.drawContours(original_img, contours_gt, 0, (0, 255, 0), 1)\n        overlap_img = cv2.drawContours(overlap_img, contours_p, 0, (255,36,0), 1)#255,0,0\n        prediction_overlap.append(overlap_img)\n\n    return prediction_overlap\n    \n    \n#prediction_overlap_u = batch_preds_overlap(unet, test_samples)\n#prediction_overlap_f = batch_preds_overlap(fpn, test_samples)\nprediction_overlap_r = batch_preds_overlap(rx50, test_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA\n\n# unet plates\n#pred_overlap_5x1_u = []\n#pred_overlap_5x3_u = []\n\n# fpn plates\n#pred_overlap_5x1_f = []\n#pred_overlap_5x3_f = []\n\n# rx plates\npred_overlap_5x1_r = []\npred_overlap_5x3_r = []\n\nfor i in range(5, 105+5, 5):\n    #pred_overlap_5x1_u.append(np.hstack(np.array(prediction_overlap_u[i-5:i])))\n    #pred_overlap_5x1_f.append(np.hstack(np.array(prediction_overlap_f[i-5:i])))\n    pred_overlap_5x1_r.append(np.hstack(np.array(prediction_overlap_r[i-5:i])))\n\nfor i in range(3, 21+3, 3):\n   #pred_overlap_5x3_u.append(np.vstack(pred_overlap_5x1_u[i-3:i]))\n   #pred_overlap_5x3_f.append(np.vstack(pred_overlap_5x1_f[i-3:i]))\n    pred_overlap_5x3_r.append(np.vstack(pred_overlap_5x1_r[i-3:i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT\n\ndef plot_plate_overlap(batch_preds, title, num):\n    plt.figure(figsize=(15, 15))\n    plt.imshow(batch_preds)\n    plt.axis(\"off\")\n\n    plt.figtext(0.76,0.75,\"Green - Ground Truth\", va=\"center\", ha=\"center\", size=20,color=\"lime\");\n    plt.figtext(0.26,0.75,\"Red - Prediction\", va=\"center\", ha=\"center\", size=20, color=\"#ff0d00\");\n    plt.suptitle(title, y=.80, fontsize=20, weight=\"bold\", color=\"#00FFDE\");\n\n    fn = \"_\".join((title+str(num)).lower().split()) + \".png\"\n    plt.savefig(fn, bbox_inches='tight', pad_inches=0.2, transparent=False, facecolor='black')\n    plt.close()\n\n    \n\ntitle1 = \"Predictions of Vanilla UNet\"\ntitle2 = \"Predictions of FPN\"\ntitle3 = \"Predictions of UNet with ResNeXt50 backbone\"\n\nfor num, batch in enumerate(pred_overlap_5x3_r):\n    plot_plate_overlap(batch,title3, num)\n    \n\n\"\"\"for num, (batch1, batch2, batch3) in enumerate(zip(\n    pred_overlap_5x3_u, pred_overlap_5x3_f, pred_overlap_5x3_r)):\n    \n    plot_plate_overlap(batch1,title1, num)   \n    plot_plate_overlap(batch2,title2, num)\n    plot_plate_overlap(batch3,title3, num)\"\"\";\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gifs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\ndef make_gif(title):\n    base_name = \"_\".join(title.lower().split())\n\n    base_len = len(base_name) \n    end_len = len(\".png\")\n    fp_in = f\"{base_name}*.png\"\n    fp_out = f\"{base_name}.gif\"\n\n    img, *imgs = [Image.open(f) \n                  for f in sorted(glob.glob(fp_in), \n                                  key=lambda x : int(x[base_len:-end_len]))]\n\n    img.save(fp=fp_out, format='GIF', append_images=imgs,\n             save_all=True, duration=1000, loop=0)\n    \n    return fp_out\n\n#fn1 = make_gif(title1)\n#fn2 = make_gif(title2)\nfn3 = make_gif(title3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Display gifs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image as Image_display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(fn3,'rb') as f:\n    display(Image_display(data=f.read(), format='png'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}